{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the original outputImages\\CRW_MY23_0010_V001_tn_xPH5olEsW2C1e8s-glazed-intensity-10-V1.jpeg\n",
    "inputImg = \"./inputImages/CRW_MY23_0010_V001_tn_xPH5olEsW2C1e8s.jpeg\"\n",
    "glazedImg = \"./outputImages/CRW_MY23_0010_V001_tn_xPH5olEsW2C1e8s-glazed-intensity-10-V1.jpeg\"\n",
    "negImgP = \"./piloutput/image.png\"\n",
    "combinedImgP = './piloutput/combined_image.png'\n",
    "original_image = Image.open(inputImg)\n",
    "glazed_image = Image.open(glazedImg)\n",
    "\n",
    "# Create a negative of the image\n",
    "negative_image = Image.eval(original_image, lambda x: 255 - x)\n",
    "\n",
    "# Save the negative image\n",
    "negative_image.save(negImgP)\n",
    "negative_image.show()\n",
    "original_image.show()\n",
    "# Close the images\n",
    "original_image.close()\n",
    "negative_image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(original_path, negative_path, output_path, alpha=0.5):\n",
    "    # Open original and negative images\n",
    "    original_image = Image.open(original_path)\n",
    "    negative_image = Image.open(negative_path)\n",
    "    \n",
    "    # Resize the negative image if necessary to match the size of the original image\n",
    "    negative_image = negative_image.resize(original_image.size)\n",
    "\n",
    "    # Blend the images\n",
    "    combined_image = Image.blend(original_image, negative_image, alpha)\n",
    "    \n",
    "    # Save the combined image\n",
    "    combined_image.save(output_path)\n",
    "    \n",
    "    # Close the images\n",
    "    original_image.close()\n",
    "    negative_image.close()\n",
    "    combined_image.close()\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "combine_images(glazedImg,negImgP, combinedImgP, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Predicted class: white stork, Ciconia ciconia\n"
     ]
    }
   ],
   "source": [
    "#img classification\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "#url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "#image = Image.open(requests.get(url, stream=True).raw)\n",
    "image = Image.open(\"./piloutput/combined_image.jpg\")\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
